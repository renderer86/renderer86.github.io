---
layout: post
title: "ChatGPT와 대화 모델의 미래: 발전과 고민 2"
icon: tech
author: 1dec3cb9fc006b1947d17967c0140533139a1a4c
permalink: 042287f8d8542f900b6d4ce4505b4841db3d9145
categories: Dialogue
tags: [ChatGPT, Dialogue_Model, Chatbot, LaMDA, Bard, NLP, Dialogue]
excerpt: 이 글에서는 ChatGPT의 발전과 그에 따른 대화 모델의 미래와 고민에 대해 기술합니다.
back_color: "#ffffff"
toc: true
show: true
series: 2
index: 5
---

* 대화 모델이 갖추어야 될 능력과 ChatGPT가 보여주는 능력
* ChatGPT의 고민
* NLP 연구자들의 깊어지는 고민
* ChatGPT를 바라보는 우리의 생각
* References
{:toc}

<br/>

# 대화 모델이 갖추어야 될 능력과 ChatGPT가 보여주는 능력

*단순한 지식 답변 대화 모델이 아니다*

![]({{"/assets/img/post/042287f8d8542f900b6d4ce4505b4841db3d9145/2-1.png"| relative_url}})
*우리가 생각하는 대화 모델의 핵심 요소*

대화 모델을 잘 만들기 위해 고려되어야 할 사항은 (1) 대화의 흐름을 잘 유지하면서 일관성 유지 하기 - *Persona-grounded Conversation*, (2) AI 윤리에 위배되지 않는 대화 하기 - *Safe Conversation*, (3) 정확한 지식을 잘 전달하기 - *Knowledge-powered Conversation*, (4) 사용자의 감정에 공감하면서 대화 하기 - *Empathetic Conversation* 등이 있습니다. 실제 빅테크 기업을 포함한 학계에서도 이 네 가지 주제에 대한 연구들을 다양한 방법으로 시도하고 있습니다. ChatGPT는 처음 공개될 당시 대화형 인터페이스를 통해 사용자와 반복적으로 소통하며 사용자가 원하는 정보를 제공해주는 역할로써 소개되고 사용되었습니다. 첫 공개 당시만 해도 정보를 제공하는 대화 모델이 최신의 정보를 반영하지 못하고 2021년까지의 정보만을 학습해서 제공할 수 있는 점은 분명한 단점으로 보였습니다. [1부](https://ncsoft.github.io/ncresearch/022d8f7905fc7e231e56a955aee112fdc3d9b011)에서 언급한 것 처럼 Hallucination 문제도 아직 완벽히 해결되지 않은 모습이었습니다. 하지만 두 달이 지나고 있는 지금, ChatGPT가 보여주는 놀라운 능력은 Instruction에 있는 것으로 평가 받고 있습니다. (저희끼리 감히 예상해보건데) Open AI에서도 ChatGPT가 Instruction을 이해하고 답변하는 능력이 (Instruction-tuning의 목적이 이것이긴 하지만) 이정도 일줄은 몰랐을 것 같습니다. Open AI가 ChatGPT를 전 세계에 무료로 공개한 이유는 수 많은 사람이 수 많은 언어로 직접 ChatGPT를 사용하며 실제 사용자들이 어떤 입력을 하는지, 잘 답변 하는지 혹은 못 하는지 등 데이터를 수집하려는 목적이 분명 있었을겁니다. 거기에 추가로 수 많은 사람의 창의적인 아이디어로 ChatGPT의 한계는 어디까지인지 검증을 받는 목적도 달성하고 있는 것으로 보입니다. 재미있는 Instruction들을 모아 놓은 깃허브 저장소도 생기기 시작했습니다[^1](ChatGPT는 Instruction만 잘 써주면 위 4가지 특징에서도 높은 수준의 대화 품질을 보여줍니다).

![]({{"/assets/img/post/042287f8d8542f900b6d4ce4505b4841db3d9145/2-2.png"| relative_url}})
*놀라운 ChatGPT의 능력 - 감정을 이해하는 대화*

<br/>

# ChatGPT의 고민

*너무 뛰어나서 오히려 문제가 되는 Role-playing*

*높은 서비스 적용 비용 문제*

이런 ChatGPT도 만능은 아닙니다. 너무 뛰어나다 보니 오히려 문제가 되는 Role-playing도 존재합니다. 바로 ChatGPT의 제약을 풀어버리는 몇몇 프롬프트가 나왔습니다. ChatGPT는 앞서 언급한 대화 모델의 중요 능력 중 하나인 AI 윤리를 지키며 대화하도록 설정되어 있지만, 사람들의 무수한 노력(?) 끝에 이 제약에서 벗어날 수 있는 방법이 나오고 있습니다(놀랍게도 프롬프트의 주요 골자는 ChatGPT에게 앞으로의 대화에서 Open AI가 주입한 AI 윤리를 따르지 말라고 시키는 것입니다. 부정적인 내용을 이 블로그에 직접 넣지는 않겠습니다). Open AI는 이런 데이터들을 지속적으로 학습해 새로운 버전으로 업데이트를 하면서 막을 테지만, 이것을 뚫어내려는 시도들은 지속적으로 있을 것입니다.

ChatGPT가 놀라운 성능을 보여주는 것은 맞지만 이를 실제 서비스에 적용하는 것은 또 다른 차원의 일이 될 것입니다. ChatGPT가 적용된 New Bing 서비스는 이용자들에게 대기 순번을 부여해 사용하게 할 만큼 아직까지 범용성을 갖추지 못했고, 실제 사용을 허가 받은 사람들도 세션 당 5번 그리고 하루에 50번의 검색만을 지원하도록 제약을 받고 있습니다(최근에는 이를 세션당 6번, 하루 60회로 증가를 검토하고 있다고 하지만 실제 적용이 되더라도 큰 차이는 아닙니다). 아무래도 175B이나 되는 큰 모델을 (서비스 모델을 만들면서 더 줄였을 수도 있지만) 안정적으로 빠른 속도로 서비스를 유지하기는 글로벌 빅테크 기업에게도 쉬운 일은 아닐겁니다. 또한 기능적으로도 빙에 적용된 ChatGPT는 일반 ChatGPT와 차이가 있습니다. 단적인 예로 의학 지식에 관련된 내용은 ChatGPT는 자연스럽게 대답하지만 빙에서는 의학적 자문을 제공하지 않도록 되어 있습니다. 일반 검색 기능에서는 해당 링크를 들어가서 정보를 읽고 소스의 신뢰도 등을 판단하는 과정을 사람들이 직접 수행하는데 반해, 요약된 정보를 제공해주는 빙 ChatGPT 검색에서는 Hallucination 문제가 발생했을 때 치명적일 수 있는 도메인은 아예 정보를 제공하지 않는 것이 현재로서는 최선이라고 판단한 모양입니다. 빙이 구글의 검색 시장을 뺏어오기 위해 이런 서비스 적용의 한계를 어떻게 뛰어 넘을지 많은 사람들이 주목하고 있습니다.

<br/>

# NLP 연구자들의 깊어지는 고민

*ChatGPT가 풀어버린 다양한 NLP 태스크들*

*Meta에서 공개한 OPT*

2018년 말 BERT가 처음 나왔을 때 NLP 연구를 하고 계셨던 분들은 그 당시의 충격이 기억에 남아 있을겁니다. 형태소, 의존구조 등 여러가지 전통적인 NLP 분석 정보를 활용하는 것 보다 BERT를 이용해 (CLS 토큰에 Softmax Classifier만 연결하는 등) 간단한 모델을 구성하는 것만으로도 성능이 훨씬 더 잘 나왔습니다. 이번에도 NLP 연구자들은 ChatGPT의 성공을 바라보면서 많은 생각과 고민을 하고 있을겁니다(물론 저희도 마찬가지입니다). 지금까지 원하는 태스크의 데이터를 잘 구성해서 Fine-tuning 과정을 거쳐 높은 성능을 달성하는 것이 목표였는데, 이제는 Instruction-tuning과 사람의 피드백을 기반으로 한 강화학습 방법이 거대 언어 모델을 사용해 문제를 해결하는 일반적인 방법으로 자리를 잡아가는 중입니다. 최근 연구들에서는 최대한 다양한 종류의 Instruction을 사용해 모델을 학습하는 것이 중요하고 Instruction-tuning을 위한 학습 데이터는 기존의 Fine-tuning처럼 많은 양의 데이터를 필요로 하지 않는 실험 결과들이 나오고 있습니다.

![]({{"/assets/img/post/042287f8d8542f900b6d4ce4505b4841db3d9145/2-3.png"| relative_url}})
*Instruction-tuning 태스크 수가 많을수록, 모델의 크기가 클 수록 높은 성능 향상 효과 (1)[^2]*

구글도 이에 발맞춰 FLAN-T5 모델을 발표했습니다. 학습 데이터의 양을 많이 만드는 일은 줄어 들었지만 모델의 크기는 크면 클 수록 높은 성능을 내고 있어서 앞으로 모델 크기에 대한 경쟁이 더욱 가속화될 전망입니다.

![]({{"/assets/img/post/042287f8d8542f900b6d4ce4505b4841db3d9145/2-4.png"| relative_url}})
*Instruction-tuning 태스크 수가 많을수록, 모델의 크기가 클 수록 높은 성능 향상 효과 (2)[^3]*

사실 이런 Instruction-tuning도 언어모델이 잘 학습된 것이 있어야 시작할 수 있습니다. 영어는 다행히 메타에서 OPT라는 초거대 언어모델을 175B 규모까지 만들어서 공개했습니다[^4](하드웨어 리소스 문제로 175B 모델이 공개되어 있다고 모두가 가져다가 사용할 수 있지는 않습니다).  그리고 OPT를 이용한 Instruction-tuning 연구도 발표했습니다[^5].

<br/>

# ChatGPT를 바라보는 우리의 생각

*실체를 드러낸 거대 언어 모델의 필요성*

*궁극적인 목표는 정말 사람같은 챗봇을 만드는 것*

ChatGPT가 공개되기 전에는 그리고 Instruction-tuning에 대한 논문들이 나오기 전에는 각 태스크 혹은 서비스에 특화된 모델들을 만들고 그것들을 잘 엮은 시스템을 구성하는 것이 목표였습니다. 아무래도 NVIDIA A100 GPU 기준으로 서비스를 구성하려면 10B 이상의 모델도 버겁기 때문입니다. 하지만 이제는 어느정도 언어모델의 크기를 키우지 않고서는 기술적 한계를 극복할 수 없는 지점에 와 있다고 생각합니다. 하지만 언어모델의 크기는 단순히 키우겠다고 마음 먹는다고 쉽게 커지지 않습니다. 데이터의 충분한 양, 문어체와 구어체의 비중, 다양한 도메인이 포함되어 있는지, 해로운 컨텐츠는 최대한 걸러내고 양질의 내용으로 구성하는 등 다양한 요소들이 언어 모델의 품질에 영향을 끼치는데, 모델 학습이 실패하면 정확히 어떤 포인트가 영향을 끼쳤는지 쉽게 파악할 수 없습니다. 이런 난관을 극복해서 큰 규모의 한국어 언어모델을 성공적으로 확보하게 되면 그 다음부터는 ChatGPT 같은 모델을 만들기 위한 기본기가 생겼다고 할 수 있습니다.

우리의 목표는 ChatGPT 처럼 뛰어난 대화 모델을 만드는 것이지만, ChatGPT와 동일한 한국어 모델을 만드는 것은 아닙니다. 사람들이 대화 모델 혹은 챗봇에 기대하는 일반적인 능력인 '사람의 말을 잘 이해하고 여러 턴에 걸쳐 자연스러운 대화를 할 수 있는 것'에는 많은 것이 담겨 있습니다. 여러가지 요소 중 '정말 사람처럼 말 하는 능력'은 어떻게 만들 수 있을지 깊게 고민하고 있습니다. 사람의 요청이 있어야지만 반응하는 것이 아니라 능동적으로 대화를 이끌어가는 기술, 대화의 맥락을 이해해서 자신의 감정을 잘 표현하는 기술, 그리고 이런 것들을 아울러 자기 자신의 페르소나를 가지고 특정한 인격이 느껴지는 대화 기술 등을 연구하고 있습니다.

ChatGPT를 쓰면 쓸 수록 다소 의기소침해지고 있지만, 아직 대화 모델이 풀어야 할 숙제는 아직 많이 남아 있기에 희망을 가지고 열심히 달려갈 예정입니다.

제목의 거창함에 비해 부족한 글 읽어주셔서 감사드립니다.

(요즘은 논문 Abstract도 ChatGPT가 많이 써준다고 합니다. 우리 블로그 제목도 부탁해!)

![]({{"/assets/img/post/042287f8d8542f900b6d4ce4505b4841db3d9145/2-5.png"| relative_url}})


<br/>

# References

[^1]: [https://github.com/f/awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts)

[^2]: [https://arxiv.org/pdf/2204.07705.pdf](https://arxiv.org/pdf/2204.07705.pdf)

[^3]: [https://arxiv.org/pdf/2210.11416.pdf](https://arxiv.org/pdf/2210.11416.pdf)

[^4]: [https://arxiv.org/pdf/2205.01068.pdf](https://arxiv.org/pdf/2205.01068.pdf)

[^5]: [https://arxiv.org/pdf/2212.12017.pdf](https://arxiv.org/pdf/2212.12017.pdf)
